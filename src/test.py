# Snippet #1 version 1.0 - GlobalSearch ê¸°ë°˜ ì „ì—­ ê²€ìƒ‰ ì‹¤í–‰ê¸°

import asyncio
import os
from pathlib import Path

import pandas as pd
import tiktoken

from graphrag.api import build_index
from graphrag.config.enums import ModelType
from graphrag.config.load_config import load_config
from graphrag.config.models.language_model_config import LanguageModelConfig
from graphrag.index.typing.pipeline_run_result import PipelineRunResult
from graphrag.language_model.manager import ModelManager
from graphrag.query.indexer_adapters import (
    read_indexer_communities,
    read_indexer_entities,
    read_indexer_reports,
)
from graphrag.query.structured_search.global_search.community_context import (
    GlobalCommunityContext,
)
from graphrag.query.structured_search.global_search.search import GlobalSearch

PROJECT_DIRECTORY = Path.cwd().resolve() / "src"
GRAPHRAG_CONFIG = load_config(PROJECT_DIRECTORY)


async def run_index():
    print("ğŸ“¦ ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘...")
    try:
        results: list[PipelineRunResult] = await build_index(config=GRAPHRAG_CONFIG)
        for r in results:
            print(
                f"âœ… ì›Œí¬í”Œë¡œìš° {r.workflow} - ìƒíƒœ: {'ì„±ê³µ' if not r.errors else f'ì˜¤ë¥˜: {r.errors}'}"
            )
    except Exception as e:
        print(f"âŒ ì¸ë±ìŠ¤ ë¹Œë“œ ì‹¤íŒ¨: {e}")
        raise


async def main():
    await run_index()
    # LLM ì„¤ì •
    print("ğŸ”§ LLM ì´ˆê¸°í™”...")
    llm_config = GRAPHRAG_CONFIG.get_language_model_config("default_chat_model")

    model = ModelManager().get_or_create_chat_model(
        name="global_search",
        model_type=llm_config.type,
        config=llm_config,
    )

    # í† í¬ë‚˜ì´ì € ì„¤ì •
    try:
        token_encoder = tiktoken.encoding_for_model(llm_config.model)
    except Exception:
        token_encoder = tiktoken.get_encoding("cl100k_base")

    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ê²°ê³¼ ë°ì´í„° ë¡œë”© ì¤‘...")
    output_dir = PROJECT_DIRECTORY / "data" / "output"
    level = 2

    try:
        community_df = pd.read_parquet(output_dir / "communities.parquet")
        entity_df = pd.read_parquet(output_dir / "entities.parquet")
        report_df = pd.read_parquet(output_dir / "community_reports.parquet")

        communities = read_indexer_communities(community_df, report_df)
        reports = read_indexer_reports(report_df, community_df, level)
        entities = read_indexer_entities(entity_df, community_df, level)

    except FileNotFoundError as e:
        print(f"âŒ Parquet íŒŒì¼ ëˆ„ë½: {e}")
        return

    # Context Builder ì„¤ì •
    context_builder = GlobalCommunityContext(
        community_reports=reports,
        communities=communities,
        entities=entities,
        token_encoder=token_encoder,
    )

    # GlobalSearch íŒŒë¼ë¯¸í„° ì •ì˜
    search_engine = GlobalSearch(
        model=model,
        context_builder=context_builder,
        token_encoder=token_encoder,
        max_data_tokens=12000,
        map_llm_params={
            "max_tokens": 1000,
            "temperature": 0.0,
            "response_format": {"type": "json_object"},
        },
        reduce_llm_params={"max_tokens": 2000, "temperature": 0.0},
        context_builder_params={
            "use_community_summary": False,
            "shuffle_data": True,
            "include_community_rank": True,
            "min_community_rank": 0,
            "community_rank_name": "rank",
            "include_community_weight": True,
            "community_weight_name": "occurrence weight",
            "normalize_community_weight": True,
            "max_tokens": 12000,
            "context_name": "Reports",
        },
        allow_general_knowledge=True,
        json_mode=True,
        concurrent_coroutines=32,
        response_type="multiple paragraphs",
    )

    # ì¿¼ë¦¬ ì‹¤í–‰
    query = "í”„ë¡ íŠ¸ ë„ì–´ ì²´ì»¤ ì»¤ë²„ ì•—ì„¸ì´ ì¥ì°©(RH)ë²•ì„ ìƒì„±í•´ì¤˜."
    print(f"ğŸ” ì „ì—­ ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘: {query}")
    try:
        result = await search_engine.search(query=query)
        print("\nğŸ“„ ê²°ê³¼:")
        print(result.response)

        print("\nğŸ“Š í†µê³„:")
        print(f"LLM í˜¸ì¶œ ìˆ˜: {result.llm_calls}")
        print(f"í”„ë¡¬í”„íŠ¸ í† í°: {result.prompt_tokens}")
        print(f"ì¶œë ¥ í† í°: {result.output_tokens}")
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    asyncio.run(main())
